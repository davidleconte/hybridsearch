
from cassandra.cluster import Cluster
from cassandra.auth import PlainTextAuthProvider
from transformers import AutoTokenizer, AutoModel
import torch
import pandas as pd
import json
from faker import Faker

# This secure connect bundle is autogenerated when you donwload your SCB,
# if yours is different update the file name below
cloud_config= {
  'secure_connect_bundle': '/Users/david.leconte/Downloads/secure-connect-hybridsearch.zip'
}

# This token json file is autogenerated when you donwload your token,
# if yours is different update the file name below
with open("/Users/david.leconte/Downloads/david.leconte@datastax.com-token.json") as f:
    secrets = json.load(f)

CLIENT_ID = secrets["clientId"]
CLIENT_SECRET = secrets["secret"]

auth_provider = PlainTextAuthProvider(CLIENT_ID, CLIENT_SECRET)
cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)
session = cluster.connect()

row = session.execute("select release_version from system.local").one()
if row:
  print(row[0])
else:
  print("An error occurred.")


# Setting the keyspace
session.set_keyspace("hybridsearch")

# Creating table
table = "hybridsearch"
session.execute(f"""
CREATE TABLE IF NOT EXISTS {table} (
movie_id int PRIMARY KEY,
title text,
description text,
genre text,
release_year int,
vector list<float>
);
""")
print("Table created successfully")

# Function to generate synthetic data
def generate_fake_data(num=2000):
    fake = Faker()
    data = pd.DataFrame(columns=["movie_id", "title", "description", "genre", "release_year"])
    for i in range(num):
        data.loc[i] = [i+1, fake.sentence(nb_words=6), fake.text(max_nb_chars=200), fake.word(), fake.year()]
    return data

# Generate synthetic data
df = generate_fake_data()

# Initializing tokenizer and model from HuggingFace transformers
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased")

# Loop through each description, tokenize, generate embeddings and store it in the "vector" column of our table
for i, row in df.iterrows():
    # Tokenizing and generating embeddings
    inputs = tokenizer(row.description, return_tensors="pt", padding=True, truncation=True)
    attention_mask = inputs["attention_mask"]
    outputs = model(**inputs)
    embeddings = outputs.last_hidden_state.mean(axis=0)
    embeddings = embeddings.detach().numpy().flatten().tolist()
    embeddings = [float(x) for x in embeddings]

    # Adding vectors to the table
    session.execute(f"INSERT INTO {table} (movie_id, title, description, genre, release_year, vector) VALUES ({row['movie_id']}, '{row['title']}', '{row['description']}', '{row['genre']}', {row['release_year']}, {embeddings})")
print("Table updated successfully with embeddings")

# Function to find similar movies based on the input query
def get_similar_movies(query, k=5):
    # generate embedding for the query
    inputs = tokenizer(row.description, return_tensors="pt", padding=True, truncation=True)
    attention_mask = inputs["attention_mask"]
    outputs = model(**inputs)
    query_vector = outputs.last_hidden_state.mean(axis=0)
    query_vector = query_vector.detach().numpy().tolist()

    # execute AstraDB vector search
    cql_query = f"""
    SELECT movie_id, title, description
    FROM {table}
    WHERE genre = 'Drama'
    AND release_year >= 2010
    AND release_year <= 2020
    AND description : 'adversity'
    ORDER BY expr(movies_vector_idx, 'vector_distance(vector, {query_vector})') ASC
    LIMIT {k}
    """

    rows = session.execute(cql_query)

    # display the results
    for row in rows:
        print("Movie ID:", row.movie_id)
        print("Title:", row.title)
        print("Description:", row.description)

# Function to create and test SAI index with a specific analyzer
def test_analyzer(analyzer_name, analyzer_options):
    # Drop the existing index if it exists
    session.execute(f"DROP INDEX IF EXISTS {table}_description_idx")

    # Create a new SAI index with the specified analyzer
    session.execute(f"""
    CREATE CUSTOM INDEX {table}_description_idx ON {table}(description) USING 'org.apache.cassandra.index.sai.StorageAttachedIndex' WITH OPTIONS = {{'index_analyzer': '{analyzer_options}'}};
    """)
    print(f"Created SAI index with {analyzer_name} analyzer")

    # Perform a search using the new analyzer
    print(f"Results for {analyzer_name} analyzer:")
    get_similar_movies("A story of a young man overcoming adversity")
    print("\n")

# Test different analyzers
test_analyzer("Standard", '{{ "tokenizer" : {{"name" : "standard"}}, "filters" : [{{"name" : "porterstem"}}] }}')
test_analyzer("Whitespace", '{{ "tokenizer" : {{"name" : "whitespace"}} }}')
test_analyzer("Simple", '{{ "tokenizer" : {{"name" : "simple"}} }}')
test_analyzer("French", '{{ "tokenizer" : {{"name" : "french"}} }}')
test_analyzer("Standard with StopFilter", '{{ "tokenizer" : {{"name" : "standard"}}, "filters" : [{{"name" : "porterstem"}}, {{"name" : "stop"}}] }}')
test_analyzer("PatternReplaceCharFilter", '{{ "tokenizer" : {{"name" : "standard"}}, "charFilters" : [{{"name" : "patternreplace", "args" : {{"pattern" : "(\\\\d)", "replacement" : ""}}}}] }}')